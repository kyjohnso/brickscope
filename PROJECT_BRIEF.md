# VisioneerLabs BrickScope - Project Brief & Decisions

**Project Lead:** Kyle  
**Date:** January 2026  
**Status:** Phase 1 - Synthetic Data Generation via Blender  
**Repository:** `visioneerlabs/brickscope`

---

## Project Overview

**VisioneerLabs BrickScope** is an AI-powered AR system that helps users find specific LEGO pieces in large, chaotic piles using computer vision and grounded object detection.

**Tagline:** "Scope out exactly what you need"

### Core Use Case
- User has massive LEGO pile (example: 3ft Ã— 2ft Ã— 2ft bucket, ~150k pieces)
- User wants to build specific set (e.g., Black Seas Barracuda #6285)
- System highlights only the pieces needed from that set
- Ignores 1000+ other pieces in the pile

### Key Innovation
**Selective Grounded Detection:** Among 150 visible pieces in camera frame, only detect/highlight the 10-50 pieces user actually needs based on text query.

---

## Strategic Decisions

### 1. Vision Models (Comparative Study)

**Primary Model: Grounding-DINO** â­  
- Purpose-built for text-grounded object detection
- Best at dense scenes with selective filtering
- Speed: 5-8 fps on RTX 3090
- Open source (Apache 2.0)
- Fine-tunable on single 3090

**Secondary Model: Florence-2 (Comparison)**  
- Vision-language foundation model
- More flexible, slower (3-5 fps)
- Research comparison potential

**MSc Research Angle:** Comparative analysis of Florence-2 vs Grounding-DINO on dense LEGO detection could be novel research contribution.

### 2. Hardware Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Linux Workstation (3090)       â”‚
â”‚  - Training (Blender + ML)      â”‚
â”‚  - Inference Server (FastAPI)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ WiFi/Network
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  iPad Air M1 (5th Gen)          â”‚
â”‚  - ARKit camera                 â”‚
â”‚  - Stream to server             â”‚
â”‚  - AR overlay rendering         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Decision:** All heavy compute on Linux 3090, iPad is thin AR client.

### 3. Platform & Licensing

- **Open Source:** Apache 2.0 license
- **Repository:** `github.com/visioneerlabs/brickscope`
- **Cross-platform training data:** Works for Grounding-DINO, Florence-2, YOLO, etc.
- **No vendor lock-in:** Train once on Linux, deploy anywhere

---

## Synthetic Data Generation Strategy

### Current Phase: Blender Addon Development

**Tool:** VS Code + Claude Code for Blender addon  
**Goal:** Generate 1M+ training images with rich annotations

### Three Scene Types

**1. Single Pieces (30% - 300k images)**
```
- 1 piece per image
- Fully visible (visibility = 1.0)
- Clean training signal
- All rotations, lighting, backgrounds
- Purpose: Foundation learning
```

**2. Multiple Pieces, No Occlusion (30% - 300k images)**
```
- 5-25 pieces per image
- Grid or scattered layout
- No overlapping bboxes
- All pieces visible (visibility = 1.0)
- Purpose: Multi-object detection training
```

**3. Piles - Dense & Occluded (40% - 400k images)**
```
- 30-150 pieces per image
- Heavy occlusion (30-90% average)
- Realistic pile physics
- Partial visibility common
- Purpose: Real-world scenario
```

### Progressive Generation Plan

```
Phase 1: 10k images (validate pipeline)
Phase 2: 100k images (baseline training)
Phase 3: 500k images (competitive performance)
Phase 4: 1M+ images (only if needed)
```

**Rationale:** Validate quality before generating millions. Diminishing returns likely after 100-500k.

---

## Data Format Specification

### Rich Intermediate Format (JSON)

**Design Principle:** Output comprehensive JSON from Blender, then write lightweight converters to model-specific formats.

```
Blender â†’ Rich JSON â†’ Converters â†’ Florence-2 / Grounding-DINO / YOLO
```

### Per-Image Annotation Structure

```json
{
  "format_version": "1.0",
  "generator": "VisioneerLabs BrickScope Blender Addon v0.1",
  
  "image_metadata": {
    "file_path": "images/pile_000001.png",
    "width": 1024,
    "height": 1024,
    "scene_id": "batch_001_scene_042",
    "scene_type": "pile" | "single_piece" | "multiple_no_occlusion",
    "difficulty": "easy" | "medium" | "hard",
    "render_samples": 128,
    "camera": {
      "position": [x, y, z],
      "rotation": [rx, ry, rz],
      "fov": 50,
      "lens_mm": 50
    },
    "lighting": {
      "hdri": "studio_small_08.exr",
      "intensity": 1.2
    }
  },
  
  "objects": [
    {
      // IDENTITY
      "piece_id": "3001",
      "color_id": "red",
      "color_name": "Bright Red",
      "category": "brick",
      "piece_name": "Brick 2x4",
      "part_number": "300121",
      
      // 2D SPATIAL (absolute pixels)
      "bbox": [120, 150, 220, 250],  // [x1, y1, x2, y2] in pixels
      "center_2d": [170, 200],
      "area_pixels": 10000,
      
      // 3D SPATIAL (Blender world space)
      "position_3d": [0.05, 0.02, 0.01],
      "rotation_3d": [15, 45, 90],
      "bbox_3d_corners": [[...], [...], ...],
      
      // VISIBILITY/OCCLUSION
      "visibility": 0.65,  // 0.0-1.0
      "is_fully_visible": false,
      "is_partially_occluded": true,
      "occluding_objects": ["3002_blue", "3003_black"],
      "distance_from_camera": 0.45,
      "z_order": 12,
      "in_frame": true,
      
      // TEXT DESCRIPTIONS (multiple variants)
      "captions": {
        "short": "red brick",
        "medium": "red 2x4 brick",
        "long": "red 2x4 LEGO brick part 3001",
        "natural": "a rectangular red LEGO brick measuring 2 by 4 studs",
        "attributes": "color: red, size: 2x4, type: brick, category: standard"
      },
      
      // METADATA
      "object_index": 0,
      "spatial_neighbors": ["3002_blue"],
      
      // OPTIONAL: Segmentation
      "segmentation_mask": "masks/pile_000001_obj_000.png",
      "segmentation_polygon": [[x1,y1], [x2,y2], ...]
    }
    // ... more objects
  ],
  
  "scene_metadata": {
    "total_pieces": 87,
    "visible_pieces": 52,
    "pieces_in_frame": 48,
    "avg_occlusion": 0.65,
    "avg_visibility": 0.35,
    "piece_count_by_category": {
      "brick": 45,
      "plate": 30,
      "slope": 12
    },
    "piece_count_by_color": {
      "red": 20,
      "blue": 15,
      "black": 10
    },
    "background_type": "wood_table",
    "difficulty_score": 0.75
  }
}
```

### Critical Data Format Rules

1. **Store absolute pixel coordinates** - Don't normalize in Blender
   - Different models need different normalizations
   - Easy to normalize later, impossible to denormalize
   
2. **Multiple caption variants** - Generate several text description styles
   - Test which works best for each model
   - Florence-2 might prefer longer captions
   - Grounding-DINO might prefer shorter

3. **Include visibility/occlusion metrics**
   - Essential for training on hard examples
   - Enables curriculum learning (easyâ†’hard)
   - Important for evaluation stratification

4. **Rich metadata enables research**
   - Difficulty scores
   - Scene statistics
   - Spatial relationships

---

## Repository Structure

```
visioneerlabs/brickscope/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE (Apache 2.0)
â”œâ”€â”€ PROJECT_BRIEF.md              # This document
â”‚
â”œâ”€â”€ blender/                       # Synthetic data generation
â”‚   â”œâ”€â”€ addons/
â”‚   â”‚   â””â”€â”€ brickscope/           # Main Blender addon
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ scene_generators/
â”‚   â”‚       â”œâ”€â”€ annotations/
â”‚   â”‚       â””â”€â”€ utils/
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ generate_dataset.py
â”‚   â”‚   â””â”€â”€ batch_render.py
â”‚   â””â”€â”€ assets/
â”‚       â”œâ”€â”€ hdris/
â”‚       â”œâ”€â”€ backgrounds/
â”‚       â””â”€â”€ ldraw_parts/
â”‚
â”œâ”€â”€ data/                          # Dataset storage (gitignored)
â”‚   â”œâ”€â”€ single_pieces/
â”‚   â”œâ”€â”€ multiple_clean/
â”‚   â”œâ”€â”€ piles/
â”‚   â”œâ”€â”€ converted/
â”‚   â””â”€â”€ metadata/
â”‚
â”œâ”€â”€ training/                      # Model training code
â”‚   â”œâ”€â”€ grounding_dino/
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ checkpoints/
â”‚   â”œâ”€â”€ florence2/
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ configs/
â”‚   â””â”€â”€ converters/
â”‚       â”œâ”€â”€ to_grounding_dino.py
â”‚       â”œâ”€â”€ to_florence2.py
â”‚       â””â”€â”€ to_coco.py
â”‚
â”œâ”€â”€ inference/                     # Production inference server
â”‚   â”œâ”€â”€ server.py                 # FastAPI server
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ api/
â”‚   â””â”€â”€ utils/
â”‚
â”œâ”€â”€ clients/                       # Client applications
â”‚   â”œâ”€â”€ ios/                      # iPad ARKit app
â”‚   â”‚   â””â”€â”€ BrickScope/
â”‚   â”œâ”€â”€ android/                  # Android app (future)
â”‚   â””â”€â”€ web/                      # Web demo
â”‚
â”œâ”€â”€ evaluation/                    # Testing & metrics
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ test_images/
â”‚
â”œâ”€â”€ docs/                          # Documentation
â”‚   â”œâ”€â”€ data_format.md
â”‚   â”œâ”€â”€ blender_addon_guide.md
â”‚   â””â”€â”€ api_reference.md
â”‚
â””â”€â”€ scripts/                       # Utility scripts
    â”œâ”€â”€ dataset_stats.py
    â”œâ”€â”€ visualize_annotations.py
    â””â”€â”€ sync_models.sh
```

---

## Dataset Organization

```
data/
â”œâ”€â”€ single_pieces/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ train/      (240k images)
â”‚   â”‚   â”œâ”€â”€ val/        (30k images)
â”‚   â”‚   â””â”€â”€ test/       (30k images)
â”‚   â””â”€â”€ annotations/    (JSON files)
â”‚
â”œâ”€â”€ multiple_clean/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ train/      (240k images)
â”‚   â”‚   â”œâ”€â”€ val/        (30k images)
â”‚   â”‚   â””â”€â”€ test/       (30k images)
â”‚   â””â”€â”€ annotations/
â”‚
â”œâ”€â”€ piles/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â”œâ”€â”€ train/      (320k images)
â”‚   â”‚   â”œâ”€â”€ val/        (40k images)
â”‚   â”‚   â””â”€â”€ test/       (40k images)
â”‚   â””â”€â”€ annotations/
â”‚
â”œâ”€â”€ converted/          (Generated by converter scripts)
â”‚   â”œâ”€â”€ florence2/
â”‚   â”‚   â”œâ”€â”€ train.json
â”‚   â”‚   â””â”€â”€ val.json
â”‚   â”œâ”€â”€ grounding_dino/
â”‚   â”‚   â”œâ”€â”€ train_annotations.json
â”‚   â”‚   â””â”€â”€ val_annotations.json
â”‚   â””â”€â”€ coco/
â”‚       â””â”€â”€ instances_train.json
â”‚
â”œâ”€â”€ masks/              (Optional: instance segmentation)
â”‚   â”œâ”€â”€ train/
â”‚   â””â”€â”€ val/
â”‚
â””â”€â”€ metadata/
    â”œâ”€â”€ piece_database.json      # All LEGO pieces catalog
    â”œâ”€â”€ color_database.json      # LDraw color definitions
    â”œâ”€â”€ scene_stats.json         # Dataset statistics
    â””â”€â”€ dataset_splits.json      # Train/val/test split info
```

---

## LEGO Asset Preparation

### LDraw Library

**Source:** https://www.ldraw.org/  
**License:** CC-BY-4.0  
**Content:** ~10,000 official LEGO part models

```bash
# Download complete library
wget https://library.ldraw.org/library/updates/complete.zip
unzip complete.zip -d ~/ldraw

# Directory structure
~/ldraw/
â”œâ”€â”€ parts/          # Individual .dat files (3001.dat, 3002.dat, etc.)
â”œâ”€â”€ p/              # Primitives
â”œâ”€â”€ models/         # Complete models
â””â”€â”€ LDConfig.ldr    # Official LEGO color definitions
```

### Initial Part Set

**Black Seas Barracuda (Set 6285):**
- ~900 total pieces
- ~100-150 unique part types
- Good starter set for prototyping

**Expansion Plan:**
- Phase 1: Black Seas Barracuda parts (~150 types)
- Phase 2: Common 500 pieces (80% coverage of most sets)
- Phase 3: Full catalog (~3000-4000 active parts)

---

## Blender Generation Requirements

### Scene Parameters by Type

**Single Pieces:**
```python
pieces_per_scene: 1
camera_distance: 15-30cm
rotations: Random all axes
lighting: Varied HDRIs
backgrounds: Wood, plastic, fabric textures
render_time: ~10 sec/image
```

**Multiple No-Occlusion:**
```python
pieces_per_scene: 5-25
layout: Grid (3x3, 4x4, 5x5) or scattered
min_spacing: 5-10mm between pieces
no_bbox_overlap: True
camera_angle: Overhead or 45Â°
render_time: ~25 sec/image
```

**Piles:**
```python
pieces_per_scene: 30-150
physics_simulation: Drop or random placement
occlusion_levels: 30-90% average
camera_angles: Overhead, 30Â°, 45Â°, 60Â°
render_time: ~60 sec/image
```

### Domain Randomization

**Essential variations:**
- Camera position/angle
- Lighting (multiple HDRIs)
- Backgrounds
- Piece rotations
- Color distributions
- Piece size distributions

**Goal:** Syntheticâ†’real domain gap minimization

---

## Distribution Control System (Visual + Programmatic)

**Implementation Status:** In development (feature/distribution-control branch)

### Overview

Visual system for controlling part and color distributions with real-time preview and programmatic API for dataset generation.

### Architecture

**Three-Phase Workflow:**

```
Phase 1: Define Distribution (UI + Sliders)
    â†“
Phase 2: Preview (Geometry Nodes - Real-time)
    â†“
Phase 3: Bake (Import Real Parts - For Rendering)
```

### Phase 1: Distribution Definition (UI)

**Part Distribution:**
- UI List with weight sliders for each part type
- Add/remove parts dynamically
- Example: `Brick 2x4 (weight: 1.0)`, `Plate 2x2 (weight: 0.7)`

**Color Distribution:**
- UI List with weight sliders for each color
- Example: `Red (weight: 1.0)`, `Blue (weight: 0.5)`

**Generation Settings:**
- Total pieces count (e.g., 100 pieces)
- Random seed (for reproducibility)

**Features:**
- Normalize weights button (sum to 1.0)
- Load defaults (common parts/colors preset)
- Save/load distribution configs (JSON)
- Real-time weight statistics display

### Phase 2: Preview Mode (Geometry Nodes)

**Purpose:** Instant visual feedback without importing actual parts

**Implementation:**
- Scatter points in volume (or on surface)
- Instance parts on points using geometry nodes
- Weight-based selection (heavier weights = more instances)
- Update in real-time as user adjusts sliders

**Benefits:**
- âœ… Instant feedback (no waiting for imports)
- âœ… Interactive exploration of distributions
- âœ… Lightweight (instances, not real objects)
- âœ… See distribution before committing to import

### Phase 3: Bake Mode (Real Object Import)

**Purpose:** Create actual LEGO parts for physics simulation and rendering

**Workflow:**
1. **Sample Distribution:**
   - Use weighted random sampling
   - Generate list of (part_id, color_id) pairs
   - Example: `[(3001, 4), (3003, 1), (3001, 4), ...]` = 50x red 2x4, 30x blue 2x2, etc.

2. **Import Parts:**
   - Call `ldraw_wrapper.py` for each unique (part_id, color_id)
   - Use `part_cache.py` for efficiency (import once, instance many)
   - Apply colors via ldr_tools_blender

3. **Place Objects:**
   - Scatter placement (random positions/rotations)
   - OR prepare for physics simulation (drop from height)

4. **Result:**
   - Real Blender objects with proper materials
   - Ready for rigid body physics
   - Ready for Cycles rendering with annotations

### Multi-Distribution Mixing

**Goal:** Combine multiple distributions for complex scenarios

**Use Cases:**

**Example 1: Set-Specific + Common Parts**
```json
{
  "layers": [
    {
      "name": "Black Seas Barracuda Set",
      "source": "rebrickable:6285",
      "count": 50,
      "weight": 1.0
    },
    {
      "name": "Common Filler Bricks",
      "parts": ["3001", "3003", "3004"],
      "colors": ["4", "1", "14"],
      "count": 150,
      "weight": 0.8
    }
  ]
}
```

**Example 2: Realistic Bucket (Multiple Sets Mixed)**
```json
{
  "layers": [
    {"name": "Set A", "count": 200, "weight": 1.0},
    {"name": "Set B", "count": 100, "weight": 0.5},
    {"name": "Random Parts", "count": 500, "weight": 0.3}
  ]
}
```

### Set-Based Import (Future Feature)

**Goal:** Automatically populate distributions from LEGO set inventories

**Data Sources:**
- Rebrickable API (https://rebrickable.com/api/)
- Parse LDraw .mpd files
- Custom set inventory JSON files

**Workflow:**
1. User enters set number (e.g., "6285")
2. Fetch set inventory from API
3. Populate distribution with:
   - All unique part types
   - Correct colors
   - Piece counts matching set
4. User can adjust weights/counts before generating

**Example Sets:**
- 6285: Black Seas Barracuda (~900 pieces, ~150 unique parts)
- 10179: Millennium Falcon UCS (~5000 pieces, ~400 unique parts)

### Data Structures

**Distribution Config (JSON):**
```json
{
  "parts": [
    {"id": "3001", "name": "Brick 2x4", "weight": 1.0},
    {"id": "3003", "name": "Brick 2x2", "weight": 0.9}
  ],
  "colors": [
    {"id": "4", "name": "Red", "weight": 1.0},
    {"id": "1", "name": "Blue", "weight": 0.5}
  ],
  "total_pieces": 100,
  "seed": 12345
}
```

**Sampling Output:**
```python
# From 100 pieces with above distribution:
[
  ("3001", "4"),  # Red 2x4
  ("3003", "1"),  # Blue 2x2
  ("3001", "4"),  # Red 2x4
  ...
  # Total: 100 (part_id, color_id) pairs
]
```

### Implementation Status

**âœ… Completed:**
- `distribution_manager.py` - Weighted sampling logic
- `distribution_properties.py` - Blender scene properties
- `distribution_operators.py` - Add/remove/normalize
- `distribution_ui.py` - UI lists with weight sliders
- UI panel with part/color lists
- Save/load distribution configs
- Preset distributions (common parts/colors)

**ğŸš§ In Progress:**
- None (ready for next phase)

**ğŸ“‹ TODO:**
- Geometry nodes preview system
- Bake functionality (sample â†’ import â†’ place)
- Multi-distribution mixing
- Set-based import (Rebrickable API)
- Distribution presets library

### Key Benefits

**For Artists:**
- Visual, intuitive control
- Instant preview
- Easy experimentation

**For ML Engineers:**
- Programmatic API (JSON configs)
- Reproducible (seed-based)
- Scalable (batch generation)

**For Researchers:**
- Controlled distributions for ablation studies
- Easy to vary part/color ratios
- Dataset diversity metrics

---

## Render Performance Calculations

### Computational Requirements

```
Estimated render times (Cycles, 128 samples, 1024px):

Single piece:    10 sec/image Ã— 300k = 833 hours
Multiple pieces: 25 sec/image Ã— 300k = 2,083 hours
Piles:          60 sec/image Ã— 400k = 6,667 hours

Total for 1M images: ~9,583 hours = 400 days (single machine)

Solutions:
1. Parallel Blender instances (10 processes = 40 days)
2. Multiple machines (10 machines = 4 days)
3. Reduce samples (64 instead of 128 = 2Ã— faster)
4. Cloud rendering (AWS/GCP spot instances)
```

### Recommended Approach

```
Start small: 10k images in 4 days (single machine, 10 parallel)
Validate quality and train initial models
Scale based on results
```

---

## Next Steps - Phase 1

### 1. Blender Addon Development (VS Code + Claude Code)

**Priority 1: Core Infrastructure**
- [ ] LDraw import functionality
- [ ] Scene type generators (single/multiple/pile)
- [ ] Camera & lighting randomization
- [ ] JSON annotation export

**Priority 2: Quality Features**
- [ ] Visibility/occlusion calculation
- [ ] Multiple caption generation
- [ ] Bbox validation (in-frame, valid coords)
- [ ] Progress tracking & resumption

**Priority 3: Scale Features**
- [ ] Batch processing
- [ ] Parallel rendering support
- [ ] Error handling & logging
- [ ] Dataset statistics generation

### 2. Validation Dataset (10k images)

**Goal:** Prove pipeline works end-to-end

```
Generate:
â”œâ”€ 3k single pieces
â”œâ”€ 3k multiple clean
â””â”€ 4k piles

Test:
â”œâ”€ Annotation quality
â”œâ”€ Visual inspection
â”œâ”€ Converter scripts (to Florence-2/Grounding-DINO formats)
â””â”€ Train tiny model to validate data works
```

### 3. Initial Model Training

**After 10k dataset ready:**
- Convert to Grounding-DINO format
- Train for 5-10 epochs (quick test)
- Validate on real LEGO pile photos
- Measure syntheticâ†’real gap
- Iterate on data quality

---

## Success Criteria - Phase 1

**Data Quality:**
- [ ] All bboxes within image bounds
- [ ] No invalid/corrupt annotations
- [ ] Visibility calculations accurate (spot check)
- [ ] Caption variety (multiple styles per object)

**Pipeline Performance:**
- [ ] Generate 10k images in <1 week (single machine)
- [ ] <5% generation failures
- [ ] Consistent quality across scene types

**Model Validation:**
- [ ] Grounding-DINO trains successfully on generated data
- [ ] >60% accuracy on synthetic test set
- [ ] Model can detect pieces in real photos (even if accuracy low)

---

## Future Phases (Post-Data Generation)

### Phase 2: Model Training & Comparison
- Train Grounding-DINO on full dataset
- Train Florence-2 on same dataset
- Comparative evaluation
- Hyperparameter tuning

### Phase 3: Deployment
- FastAPI inference server on Linux
- iPad ARKit client app
- WebSocket streaming
- Real-time AR overlay

### Phase 4: Research & Publication
- MSc thesis write-up
- Conference paper submission
- Open-source release
- Dataset publication

---

## Key Resources

**Models:**
- Grounding-DINO: https://github.com/IDEA-Research/GroundingDINO
- Florence-2: https://huggingface.co/microsoft/Florence-2-large

**Data:**
- LDraw Parts: https://www.ldraw.org/
- Rebrickable API: https://rebrickable.com/api/
- HDRIs: https://polyhaven.com/hdris

**Hardware:**
- Linux Workstation: RTX 3090 (24GB)
- Client Device: iPad Air 5th Gen (M1)

**Repository:**
- GitHub: https://github.com/visioneerlabs/brickscope

---

## Notes & Decisions Log

**2026-01-03:**
- âœ… Decided on Grounding-DINO as primary model (vs Florence-2)
- âœ… Chose 3090-centric architecture (no on-device inference)
- âœ… Rich intermediate JSON format (convert to model formats later)
- âœ… Three scene types: single/multiple/piles (30/30/40 distribution)
- âœ… Start with 10k validation set before scaling to 1M
- âœ… No Facebook/Meta products (ruled out Quest headsets)
- âœ… Focus on Linux + iPad ecosystem
- âœ… Project name: BrickScope (better than BrickFinder)
- âœ… Repository: visioneerlabs/brickscope

**Key Insight:** Grounded detection (text-based filtering) is killer feature for this use case - don't need to detect all 150 pieces, just the 10-50 user actually needs. The "-scope" metaphor perfectly captures the AR viewing/filtering experience.

---

## Questions for Claude Code Session

1. Best approach for LDraw â†’ Blender import (existing addon vs custom)?
2. Optimal Blender scene structure for parallel rendering?
3. Physics simulation vs manual placement for piles?
4. Segmentation mask generation worth the complexity?
5. HDRI licensing for commercial use?

---

**End of Brief - Ready for VS Code + Claude Code Implementation** ğŸ”ğŸ§±

**Repository:** `github.com/visioneerlabs/brickscope`  
**License:** Apache 2.0  
**Status:** Phase 1 - Blender Addon Development
